{"cells":[{"cell_type":"markdown","metadata":{"id":"1oHFCsV0z-Jw"},"source":["# Finetune Llama-3 with LLaMA Factory\n","\n","Please use a **free** Tesla T4 Colab GPU to run this!\n","\n","Project homepage: https://github.com/hiyouga/LLaMA-Factory\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lr7rB3szzhtx"},"source":["## Install Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1740969381119,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"},"user_tz":360},"id":"giM74oK1rRIH","outputId":"64c02d8a-c161-4235-ec09-495413b5952c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/work/09986/dchen1616/LLaMA-Factory\n"]}],"source":["#%cd /content/\n","#%rm -rf LLaMA-Factory\n","#!git clone https://github.com/hiyouga/LLaMA-Factory.git\n","#%cd LLaMA-Factory\n","#%ls\n","#!pip install -e .[torch,bitsandbytes]\n","%pwd\n","%cd /home1/09986/dchen1616/work/LLaMA-Factory"]},{"cell_type":"markdown","source":["## dependency\n"],"metadata":{"id":"jtbGS0mL4YoT"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1723,"status":"ok","timestamp":1740969385406,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"},"user_tz":360},"id":"lamyQ74pZy44","outputId":"b40b5687-2c47-4b06-98f3-297220b88b21"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: requests 2.32.3\r\n","Uninstalling requests-2.32.3:\r\n","  Successfully uninstalled requests-2.32.3\r\n","Found existing installation: pyarrow 19.0.1\n","Uninstalling pyarrow-19.0.1:\n","  Successfully uninstalled pyarrow-19.0.1\n","Found existing installation: fsspec 2024.6.1\n","Uninstalling fsspec-2024.6.1:\n","  Successfully uninstalled fsspec-2024.6.1\n"]}],"source":["!pip uninstall -y requests pyarrow fsspec"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8735,"status":"ok","timestamp":1740969395853,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"},"user_tz":360},"id":"axIWLzmIgNPJ","outputId":"8da6311f-e55d-4dba-ae05-99bd0160f76c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting requests==2.31.0\r\n","  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from requests==2.31.0) (3.4.1)\r\n","Requirement already satisfied: idna<4,>=2.5 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from requests==2.31.0) (3.10)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from requests==2.31.0) (2.3.0)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from requests==2.31.0) (2025.1.31)\n","Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n","Installing collected packages: requests\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datasets 3.2.0 requires fsspec[http]<=2024.9.0,>=2023.1.0, which is not installed.\n","datasets 3.2.0 requires pyarrow>=15.0.0, which is not installed.\n","huggingface-hub 0.29.1 requires fsspec>=2023.5.0, which is not installed.\n","datasets 3.2.0 requires requests>=2.32.2, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed requests-2.31.0\n","Collecting pyarrow==14.0.1\n","  Using cached pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: numpy>=1.16.6 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from pyarrow==14.0.1) (1.26.4)\n","Using cached pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n","Installing collected packages: pyarrow\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datasets 3.2.0 requires fsspec[http]<=2024.9.0,>=2023.1.0, which is not installed.\n","datasets 3.2.0 requires pyarrow>=15.0.0, but you have pyarrow 14.0.1 which is incompatible.\n","datasets 3.2.0 requires requests>=2.32.2, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pyarrow-14.0.1\n","Collecting fsspec==2024.6.1\n","  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n","Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n","Installing collected packages: fsspec\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datasets 3.2.0 requires pyarrow>=15.0.0, but you have pyarrow 14.0.1 which is incompatible.\n","datasets 3.2.0 requires requests>=2.32.2, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed fsspec-2024.6.1\n"]}],"source":["!pip install requests==2.31.0\n","!pip install pyarrow==14.0.1\n","!pip install fsspec==2024.6.1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1688,"status":"ok","timestamp":1740969401032,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"},"user_tz":360},"id":"FxHBAOJdgdZD","outputId":"b6839992-8069-4b6c-982c-37e2a8ff8648"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests_mock in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (1.12.1)\r\n","Requirement already satisfied: pyqt5 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (5.15.11)\r\n","Requirement already satisfied: pyqtwebengine in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (5.15.7)\r\n","Requirement already satisfied: requests<3,>=2.22 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from requests_mock) (2.31.0)\r\n","Requirement already satisfied: PyQt5-sip<13,>=12.15 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from pyqt5) (12.17.0)\r\n","Requirement already satisfied: PyQt5-Qt5<5.16.0,>=5.15.2 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from pyqt5) (5.15.16)\r\n","Requirement already satisfied: PyQtWebEngine-Qt5<5.16.0,>=5.15.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from pyqtwebengine) (5.15.16)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from requests<3,>=2.22->requests_mock) (3.4.1)\r\n","Requirement already satisfied: idna<4,>=2.5 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from requests<3,>=2.22->requests_mock) (3.10)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from requests<3,>=2.22->requests_mock) (2.3.0)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from requests<3,>=2.22->requests_mock) (2025.1.31)\r\n"]}],"source":["!pip install requests_mock pyqt5 pyqtwebengine\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11948,"status":"ok","timestamp":1740969416887,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"},"user_tz":360},"id":"gFTuu_z8gifK","outputId":"da891432-f063-4888-826f-3faada56a700"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing /work/09986/dchen1616/LLaMA-Factory\n","  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\n","\u001b[?25hRequirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (4.49.0)\n","Requirement already satisfied: datasets<=3.2.0,>=2.16.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (3.2.0)\n","Requirement already satisfied: accelerate<=1.2.1,>=0.34.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (1.2.1)\n","Requirement already satisfied: peft<=0.12.0,>=0.11.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (0.12.0)\n","Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (0.9.6)\n","Requirement already satisfied: tokenizers<=0.21.0,>=0.19.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (0.21.0)\n","Requirement already satisfied: gradio<=5.18.0,>=4.38.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (5.18.0)\n","Requirement already satisfied: pandas>=2.0.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (2.2.3)\n","Requirement already satisfied: scipy in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (1.15.2)\n","Requirement already satisfied: einops in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (0.8.1)\n","Requirement already satisfied: sentencepiece in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (0.2.0)\n","Requirement already satisfied: tiktoken in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (0.9.0)\n","Requirement already satisfied: protobuf in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (5.29.3)\n","Requirement already satisfied: uvicorn in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (0.34.0)\n","Requirement already satisfied: pydantic in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (2.10.6)\n","Requirement already satisfied: fastapi in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (0.115.11)\n","Requirement already satisfied: sse-starlette in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (2.2.1)\n","Requirement already satisfied: matplotlib>=3.7.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (3.10.1)\n","Requirement already satisfied: fire in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (0.7.0)\n","Requirement already satisfied: packaging in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (24.2)\n","Requirement already satisfied: pyyaml in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (6.0.2)\n","Requirement already satisfied: numpy<2.0.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (1.26.4)\n","Requirement already satisfied: av in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (14.2.0)\n","Requirement already satisfied: librosa in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (0.10.2.post1)\n","Requirement already satisfied: tyro<0.9.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (0.8.14)\n","Requirement already satisfied: torch>=1.13.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (2.6.0)\n","Collecting bitsandbytes>=0.39.0 (from llamafactory==0.9.2.dev0)\n","  Using cached bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: psutil in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from accelerate<=1.2.1,>=0.34.0->llamafactory==0.9.2.dev0) (7.0.0)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from accelerate<=1.2.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.29.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from accelerate<=1.2.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.5.3)\n","Requirement already satisfied: filelock in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.17.0)\n","Collecting pyarrow>=15.0.0 (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0)\n","  Using cached pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.3.8)\n","Collecting requests>=2.32.2 (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0)\n","  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: tqdm>=4.66.3 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (4.67.1)\n","Requirement already satisfied: xxhash in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (2024.6.1)\n","Requirement already satisfied: aiohttp in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.11.13)\n","Requirement already satisfied: aiofiles<24.0,>=22.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (23.2.1)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (4.8.0)\n","Requirement already satisfied: ffmpy in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (0.5.0)\n","Requirement already satisfied: gradio-client==1.7.2 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (1.7.2)\n","Requirement already satisfied: httpx>=0.24.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (0.28.1)\n","Requirement already satisfied: jinja2<4.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (3.1.5)\n","Requirement already satisfied: markupsafe~=2.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (2.1.5)\n","Requirement already satisfied: orjson~=3.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (3.10.15)\n","Requirement already satisfied: pillow<12.0,>=8.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (11.1.0)\n","Requirement already satisfied: pydub in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.18 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (0.0.20)\n","Requirement already satisfied: ruff>=0.9.3 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (0.9.9)\n","Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (0.1.6)\n","Requirement already satisfied: semantic-version~=2.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (2.10.0)\n","Requirement already satisfied: starlette<1.0,>=0.40.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (0.46.0)\n","Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (0.13.2)\n","Requirement already satisfied: typer<1.0,>=0.12 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (0.15.2)\n","Requirement already satisfied: typing-extensions~=4.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (4.12.2)\n","Requirement already satisfied: websockets<16.0,>=10.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from gradio-client==1.7.2->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (15.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2025.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from pydantic->llamafactory==0.9.2.dev0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from pydantic->llamafactory==0.9.2.dev0) (2.27.2)\n","Requirement already satisfied: networkx in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (3.4.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.2.dev0) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->llamafactory==0.9.2.dev0) (2024.11.6)\n","Requirement already satisfied: docstring-parser>=0.16 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (0.16)\n","Requirement already satisfied: rich>=11.1.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (13.9.4)\n","Requirement already satisfied: shtab>=1.5.6 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (1.7.1)\n","Requirement already satisfied: click>=7.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from uvicorn->llamafactory==0.9.2.dev0) (8.1.8)\n","Requirement already satisfied: h11>=0.8 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from uvicorn->llamafactory==0.9.2.dev0) (0.14.0)\n","Requirement already satisfied: termcolor in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from fire->llamafactory==0.9.2.dev0) (2.5.0)\n","Requirement already satisfied: audioread>=2.1.9 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from librosa->llamafactory==0.9.2.dev0) (3.0.1)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from librosa->llamafactory==0.9.2.dev0) (1.6.1)\n","Requirement already satisfied: joblib>=0.14 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from librosa->llamafactory==0.9.2.dev0) (1.4.2)\n","Requirement already satisfied: decorator>=4.3.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from librosa->llamafactory==0.9.2.dev0) (5.1.1)\n","Requirement already satisfied: numba>=0.51.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from librosa->llamafactory==0.9.2.dev0) (0.61.0)\n","Requirement already satisfied: soundfile>=0.12.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from librosa->llamafactory==0.9.2.dev0) (0.13.1)\n","Requirement already satisfied: pooch>=1.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from librosa->llamafactory==0.9.2.dev0) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from librosa->llamafactory==0.9.2.dev0) (0.5.0.post1)\n","Requirement already satisfied: lazy-loader>=0.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from librosa->llamafactory==0.9.2.dev0) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from librosa->llamafactory==0.9.2.dev0) (1.1.0)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (1.2.0)\n","Requirement already satisfied: idna>=2.8 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (1.3.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (5.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.18.3)\n","Requirement already satisfied: certifi in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (1.0.7)\n","Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from numba>=0.51.0->librosa->llamafactory==0.9.2.dev0) (0.44.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from pooch>=1.1->librosa->llamafactory==0.9.2.dev0) (4.3.6)\n","Requirement already satisfied: six>=1.5 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from requests>=2.32.2->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from requests>=2.32.2->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.3.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (2.15.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa->llamafactory==0.9.2.dev0) (3.5.0)\n","Requirement already satisfied: cffi>=1.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.2.dev0) (1.17.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (1.5.4)\n","Requirement already satisfied: pycparser in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.2.dev0) (2.22)\n","Requirement already satisfied: mdurl~=0.1 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (0.1.2)\n","Using cached bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n","Using cached pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n","Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n","Building wheels for collected packages: llamafactory\n","  Building wheel for llamafactory (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n","\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.2.dev0-py3-none-any.whl size=278547 sha256=6e23958fa416f09e80c09d2ad93cef08af249dd71a969e2896ba3adbb6f9eb56\n","  Stored in directory: /home1/09986/dchen1616/.cache/pip/wheels/bd/33/5c/b8c322776a81739e79c5280d9ea6c8c06ac8d75d0c6eb8287a\n","Successfully built llamafactory\n","Installing collected packages: requests, pyarrow, bitsandbytes, llamafactory\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.1\n","    Uninstalling pyarrow-14.0.1:\n","      Successfully uninstalled pyarrow-14.0.1\n","  Attempting uninstall: bitsandbytes\n","    Found existing installation: bitsandbytes 0.37.2\n","    Uninstalling bitsandbytes-0.37.2:\n","      Successfully uninstalled bitsandbytes-0.37.2\n","  Attempting uninstall: llamafactory\n","    Found existing installation: llamafactory 0.9.2.dev0\n","    Uninstalling llamafactory-0.9.2.dev0:\n","      Successfully uninstalled llamafactory-0.9.2.dev0\n","Successfully installed bitsandbytes-0.45.3 llamafactory-0.9.2.dev0 pyarrow-19.0.1 requests-2.32.3\n"]}],"source":["!pip install \".[torch,bitsandbytes]\""]},{"cell_type":"markdown","metadata":{"id":"H9RXn_YQnn9f"},"source":["### Check GPU environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZkN-ktlsnrdU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740969426151,"user_tz":360,"elapsed":2,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"}},"outputId":"a9fdfb05-8e01-4337-9ee4-d37568e19ae8"},"outputs":[{"output_type":"stream","name":"stdout","text":["NVIDIA A100-PCIE-40GB\n","Total GPUs:3\n"]}],"source":["import torch\n","try:\n","  assert torch.cuda.is_available() is True\n","  print(torch.cuda.get_device_name())\n","  print(\"Total GPUs:\"+ str(torch.cuda.device_count()))\n","except AssertionError:\n","  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"]},{"cell_type":"markdown","metadata":{"id":"TeYs5Lz-QJYk"},"source":["## Update Identity Dataset"]},{"cell_type":"code","source":["%pwd\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"0RwA5LoOk57k","executionInfo":{"status":"ok","timestamp":1744320057909,"user_tz":300,"elapsed":23,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"}},"outputId":"e0d0bde7-a21c-48ea-93df-7c47af5d50f8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ap_fvMBsQHJc","executionInfo":{"status":"ok","timestamp":1744337487378,"user_tz":300,"elapsed":58,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e01577c-7ae0-45e8-bfc8-90800eb59859"},"outputs":[{"output_type":"stream","name":"stdout","text":["/work/09986/dchen1616/LLaMA-Factory/data/New\n"]}],"source":["import json\n","\n","#%cd /content/LLaMA-Factory/\n","%cd /home1/09986/dchen1616/work/LLaMA-Factory/data/New\n","%pwd\n","NAME = \"Llama-3\"\n","AUTHOR = \"LLaMA Factory\"\n","#with open(\"data/output.json\", \"r\", encoding=\"utf-8\") as f:\n","with open(\"train_cot_final.json\", \"r\", encoding=\"utf-8\") as f:\n","  dataset = json.load(f)\n","\n","for sample in dataset:\n","  sample[\"output\"] = str(sample[\"output\"]).replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n","\n","with open(\"output_cot_final.json\", \"w\", encoding=\"utf-8\") as f:\n","  json.dump(dataset, f, indent=4, ensure_ascii=False)"]},{"cell_type":"markdown","metadata":{"id":"2QiXcvdzzW3Y"},"source":["## Fine-tune model via LLaMA Board"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3119,"status":"ok","timestamp":1744838411523,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"},"user_tz":300},"id":"IxF2TmIfbn4i","outputId":"d494c037-6ef2-4e5c-a757-4c83c6a64de1"},"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","The token `token` has been saved to /home1/09986/dchen1616/.cache/huggingface/stored_tokens\n","Your token has been saved to /home1/09986/dchen1616/.cache/huggingface/token\n","Login successful.\n","The current active token is: `token`\n"]}],"source":["!huggingface-cli login --token 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"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YLsdS6V5yUMy","outputId":"9521c442-8b1f-477b-832f-58a3da5ec341","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["/work/09986/dchen1616/LLaMA-Factory\n","which: no node in (/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/bin:/home1/09986/dchen1616/work/anaconda3/condabin:/home1/09986/dchen1616/work/anaconda3/bin:/home1/09986/dchen1616/.vscode-server/bin/441438abd1ac652551dbe4d408dfcec8a499b8bf/bin/remote-cli:/home1/09986/dchen1616/work/anaconda3/bin:/home1/09986/dchen1616/work/anaconda3/bin:/opt/apps/xalt/xalt/bin:/opt/apps/pmix/3.2.3/bin:/opt/apps/cmake/3.24.2/bin:/opt/apps/intel19/python3/3.9.7/bin:/opt/apps/autotools/1.4/bin:/opt/intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/bin:/opt/intel/compilers_and_libraries_2020.1.217/linux/bin/intel64:/opt/apps/gcc/9.4.0/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:.)\n","* Running on local URL:  http://0.0.0.0:7860\n","* Running on public URL: https://64ac91aab89444996d.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","Unable to connect to VS Code server: Error in request.\n","Error: connect ENOENT /run/user/893351/vscode-ipc-5c7bd678-d5e7-4b92-9ea7-5224845fce0c.sock\n","\u001b[90m    at PipeConnectWrap.afterConnect [as oncomplete] (node:net:1157:16)\u001b[39m {\n","  errno: \u001b[33m-2\u001b[39m,\n","  code: \u001b[32m'ENOENT'\u001b[39m,\n","  syscall: \u001b[32m'connect'\u001b[39m,\n","  address: \u001b[32m'/run/user/893351/vscode-ipc-5c7bd678-d5e7-4b92-9ea7-5224845fce0c.sock'\u001b[39m\n","}\n","[INFO|configuration_utils.py:697] 2025-04-16 16:32:32,425 >> loading configuration file /home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-04-16 16:32:32,426 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"/home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:32:32,428 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:32:32,428 >> loading file tokenizer.model\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:32:32,428 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:32:32,428 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:32:32,428 >> loading file tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:32:32,428 >> loading file chat_template.jinja\n","[INFO|tokenization_utils_base.py:2313] 2025-04-16 16:32:32,700 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|configuration_utils.py:697] 2025-04-16 16:32:32,704 >> loading configuration file /home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-04-16 16:32:32,704 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"/home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:32:32,706 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:32:32,706 >> loading file tokenizer.model\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:32:32,706 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:32:32,706 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:32:32,706 >> loading file tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:32:32,706 >> loading file chat_template.jinja\n","[INFO|tokenization_utils_base.py:2313] 2025-04-16 16:32:32,970 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|2025-04-16 16:32:32] llamafactory.data.template:157 >> Add pad token: <|eot_id|>\n","[INFO|2025-04-16 16:32:32] llamafactory.data.template:157 >> Add <|eot_id|>,<|eom_id|> to stop words.\n","[INFO|configuration_utils.py:697] 2025-04-16 16:32:32,986 >> loading configuration file /home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-04-16 16:32:32,987 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"/home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|2025-04-16 16:32:32] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n","[INFO|modeling_utils.py:3979] 2025-04-16 16:32:33,117 >> loading weights file /home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct/model.safetensors.index.json\n","[INFO|modeling_utils.py:1633] 2025-04-16 16:32:33,118 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n","[INFO|configuration_utils.py:1140] 2025-04-16 16:32:33,119 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ]\n","}\n","\n","Loading checkpoint shards: 100%|██████████████████| 4/4 [00:13<00:00,  3.33s/it]\n","[INFO|modeling_utils.py:4970] 2025-04-16 16:32:48,209 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","[INFO|modeling_utils.py:4978] 2025-04-16 16:32:48,209 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","[INFO|configuration_utils.py:1093] 2025-04-16 16:32:48,246 >> loading configuration file /home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-04-16 16:32:48,246 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","[INFO|2025-04-16 16:32:48] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n","Traceback (most recent call last):\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/peft/config.py\", line 205, in _get_peft_type\n","    config_file = hf_hub_download(\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n","    validate_repo_id(arg_value)\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\n","    raise HFValidationError(\n","huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'saves/Llama-3-8B-Instruct/lora/train_cot_final_20250410'. Use `repo_type` argument if needed.\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/gradio/queueing.py\", line 715, in process_events\n","    response = await route_utils.call_process_api(\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n","    output = await app.get_blocks().process_api(\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/gradio/blocks.py\", line 2096, in process_api\n","    result = await self.call_function(\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/gradio/blocks.py\", line 1655, in call_function\n","    prediction = await utils.async_iteration(iterator)\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/gradio/utils.py\", line 735, in async_iteration\n","    return await anext(iterator)\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/gradio/utils.py\", line 729, in __anext__\n","    return await anyio.to_thread.run_sync(\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n","    return await get_async_backend().run_sync_in_worker_thread(\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n","    return await future\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n","    result = context.run(func, *args)\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/gradio/utils.py\", line 712, in run_sync_iterator_async\n","    return next(iterator)\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/gradio/utils.py\", line 873, in gen_wrapper\n","    response = next(iterator)\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/llamafactory/webui/chatter.py\", line 135, in load_model\n","    super().__init__(args)\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/llamafactory/chat/chat_model.py\", line 52, in __init__\n","    self.engine: \"BaseEngine\" = HuggingfaceEngine(model_args, data_args, finetuning_args, generating_args)\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/llamafactory/chat/hf_engine.py\", line 59, in __init__\n","    self.model = load_model(\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/llamafactory/model/loader.py\", line 169, in load_model\n","    model = init_adapter(config, model, model_args, finetuning_args, is_trainable)\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/llamafactory/model/adapter.py\", line 299, in init_adapter\n","    model = _setup_lora_tuning(\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/llamafactory/model/adapter.py\", line 181, in _setup_lora_tuning\n","    model: \"LoraModel\" = PeftModel.from_pretrained(model, adapter, **init_kwargs)\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/peft/peft_model.py\", line 453, in from_pretrained\n","    PeftConfig._get_peft_type(\n","  File \"/home1/09986/dchen1616/work/anaconda3/envs/llamafactory/lib/python3.10/site-packages/peft/config.py\", line 211, in _get_peft_type\n","    raise ValueError(f\"Can't find '{CONFIG_NAME}' at '{model_id}'\")\n","ValueError: Can't find 'adapter_config.json' at 'saves/Llama-3-8B-Instruct/lora/train_cot_final_20250410'\n","[WARNING|2025-04-16 16:34:16] llamafactory.webui.common:162 >> Found complex path, some features may be not available.\n","[INFO|configuration_utils.py:697] 2025-04-16 16:34:16,044 >> loading configuration file /home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-04-16 16:34:16,044 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"/home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:34:16,046 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:34:16,046 >> loading file tokenizer.model\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:34:16,046 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:34:16,046 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:34:16,046 >> loading file tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:34:16,046 >> loading file chat_template.jinja\n","[INFO|tokenization_utils_base.py:2313] 2025-04-16 16:34:16,301 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|configuration_utils.py:697] 2025-04-16 16:34:16,304 >> loading configuration file /home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-04-16 16:34:16,305 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"/home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:34:16,308 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:34:16,308 >> loading file tokenizer.model\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:34:16,308 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:34:16,308 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:34:16,308 >> loading file tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-16 16:34:16,308 >> loading file chat_template.jinja\n","[INFO|tokenization_utils_base.py:2313] 2025-04-16 16:34:16,560 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|2025-04-16 16:34:16] llamafactory.data.template:157 >> Add pad token: <|eot_id|>\n","[INFO|2025-04-16 16:34:16] llamafactory.data.template:157 >> Add <|eot_id|>,<|eom_id|> to stop words.\n","[INFO|configuration_utils.py:697] 2025-04-16 16:34:16,574 >> loading configuration file /home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-04-16 16:34:16,575 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"/home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|2025-04-16 16:34:16] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n","[INFO|modeling_utils.py:3979] 2025-04-16 16:34:16,576 >> loading weights file /home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct/model.safetensors.index.json\n","[INFO|modeling_utils.py:1633] 2025-04-16 16:34:16,577 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n","[INFO|configuration_utils.py:1140] 2025-04-16 16:34:16,578 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ]\n","}\n","\n","Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.33s/it]\n","[INFO|modeling_utils.py:4970] 2025-04-16 16:34:21,940 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","[INFO|modeling_utils.py:4978] 2025-04-16 16:34:21,940 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","[INFO|configuration_utils.py:1093] 2025-04-16 16:34:21,959 >> loading configuration file /home1/09986/dchen1616/work/LLaMA-Factory/Meta-Llama-3.1-8B-Instruct/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-04-16 16:34:21,959 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","[INFO|2025-04-16 16:34:22] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n","[INFO|2025-04-16 16:34:29] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n","[INFO|2025-04-16 16:34:29] llamafactory.model.adapter:157 >> Loaded adapter(s): /home1/09986/dchen1616/work/LLaMA-Factory/saves/Llama-3.1-8B-Instruct/lora/train_cot_final_20250410\n","[INFO|2025-04-16 16:34:29] llamafactory.model.loader:157 >> all params: 8,030,261,248\n","[WARNING|2025-04-16 16:34:29] llamafactory.chat.hf_engine:168 >> There is no current event loop, creating a new one.\n"]}],"source":["#%cd /content/LLaMA-Factory/\n","%cd /home1/09986/dchen1616/work/LLaMA-Factory\n","#!huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --local-dir Meta-Llama-3.1-8B-Instruct\n","!GRADIO_SHARE=1 llamafactory-cli webui"]},{"cell_type":"code","source":["!pip install jieba\n","!pip install rouge-chinese\n","!pip install nltk"],"metadata":{"id":"BYjO1MVsb17y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741146749146,"user_tz":360,"elapsed":7396,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"}},"outputId":"71c2dc7c-cea4-46f6-d78d-7d020c522f0f","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: jieba in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (0.42.1)\n","Requirement already satisfied: rouge-chinese in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (1.0.3)\n","Requirement already satisfied: six in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from rouge-chinese) (1.16.0)\n","Requirement already satisfied: nltk in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (3.9.1)\n","Requirement already satisfied: click in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /work/09986/dchen1616/anaconda3/envs/llamafactory/lib/python3.10/site-packages (from nltk) (4.67.1)\n"]}]},{"cell_type":"code","source":["import json\n","%cd /home1/09986/dchen1616/work/LLaMA-Factory\n","NAME = \"Llama-3\"\n","AUTHOR = \"LLaMA Factory\"\n","#with open(\"data/output.json\", \"r\", encoding=\"utf-8\") as f:\n","with open(\"./data/New/valid_cot_update.json\", \"r\", encoding=\"utf-8\") as f:\n","  dataset = json.load(f)\n","\n","for sample in dataset:\n","  sample[\"output\"] = str(sample[\"output\"]).replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n","\n","with open(\"./data/New/output_cot_validate_update.json\", \"w\", encoding=\"utf-8\") as f:\n","  json.dump(dataset, f, indent=4, ensure_ascii=False)"],"metadata":{"id":"LJX2TCmQb2hq","colab":{"base_uri":"https://localhost:8080/","height":233},"executionInfo":{"status":"error","timestamp":1744337447309,"user_tz":300,"elapsed":42,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"}},"outputId":"ceda4824-c0df-4626-9163-b0930cc9633c","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/work/09986/dchen1616/LLaMA-Factory\n"]},{"output_type":"error","ename":"KeyError","evalue":"'output'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m   dataset \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m---> 10\u001b[0m   sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}}\u001b[39m\u001b[38;5;124m\"\u001b[39m, NAME)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}}\u001b[39m\u001b[38;5;124m\"\u001b[39m, AUTHOR)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/New/output_cot_test_final.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     13\u001b[0m   json\u001b[38;5;241m.\u001b[39mdump(dataset, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","\u001b[0;31mKeyError\u001b[0m: 'output'"]}]},{"cell_type":"code","source":["import json\n","%cd /home1/09986/dchen1616/work/LLaMA-Factory\n","NAME = \"Llama-3\"\n","AUTHOR = \"LLaMA Factory\"\n","#with open(\"data/output.json\", \"r\", encoding=\"utf-8\") as f:\n","with open(\"./data/New/test_cot_final.json\", \"r\", encoding=\"utf-8\") as f:\n","  dataset = json.load(f)\n","\n","for sample in dataset:\n","  sample[\"output\"] = sample.get(\"output\", \"\")\n","  sample[\"input\"] = str(sample[\"input\"]).replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n","\n","with open(\"./data/New/output_cot_test_final.json\", \"w\", encoding=\"utf-8\") as f:\n","  json.dump(dataset, f, indent=4, ensure_ascii=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fIB4NsCtUh7t","executionInfo":{"status":"ok","timestamp":1744384156101,"user_tz":300,"elapsed":127,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"}},"outputId":"bdf13aac-3cab-4822-d706-8f13e7fbef5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/work/09986/dchen1616/LLaMA-Factory\n"]}]},{"cell_type":"markdown","source":["# Prediction of training and validation dataset with sft"],"metadata":{"id":"GFEgw3iX4nns"}},{"cell_type":"code","source":["%cd /home1/09986/dchen1616/work/LLaMA-Factory\n","!CUDA_VISIBLE_DEVICES=0,1,2\n","!llamafactory-cli train \\\n","    --stage sft \\\n","    --do_predict \\\n","    --model_name_or_path \"Meta-Llama-3.1-8B-Instruct\" \\\n","    --adapter_name_or_path \"saves/Llama-3.1-8B-Instruct/lora/train_cot_final_20250410\"  \\\n","    --eval_dataset output_cot_final \\\n","    --dataset_dir ./data/New \\\n","    --template llama3 \\\n","    --finetuning_type lora \\\n","    --output_dir \"saves/LLaMA3-8B/lora/predict_train_cot_final\" \\\n","    --overwrite_cache \\\n","    --overwrite_output_dir \\\n","    --cutoff_len 2048 \\\n","    --preprocessing_num_workers 16 \\\n","    --per_device_eval_batch_size 2 \\\n","    --max_samples 100000 \\\n","    --predict_with_generate"],"metadata":{"id":"SwBikMyAcUJH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744383952667,"user_tz":300,"elapsed":274633,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"}},"outputId":"89c9c952-4c03-4016-9e6e-e719b162cbbb","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/work/09986/dchen1616/LLaMA-Factory\n","[INFO|2025-04-11 10:01:27] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:26111\n","[WARNING|2025-04-11 10:01:37] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n","[INFO|2025-04-11 10:01:37] llamafactory.hparams.parser:384 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: None\n","[INFO|configuration_utils.py:697] 2025-04-11 10:01:37,127 >> loading configuration file Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-04-11 10:01:37,128 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:01:37,131 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:01:37,131 >> loading file tokenizer.model\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:01:37,131 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:01:37,131 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:01:37,131 >> loading file tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:01:37,131 >> loading file chat_template.jinja\n","[INFO|tokenization_utils_base.py:2313] 2025-04-11 10:01:37,445 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|configuration_utils.py:697] 2025-04-11 10:01:37,447 >> loading configuration file Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-04-11 10:01:37,448 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:01:37,450 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:01:37,450 >> loading file tokenizer.model\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:01:37,450 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:01:37,450 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:01:37,450 >> loading file tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:01:37,450 >> loading file chat_template.jinja\n","[INFO|2025-04-11 10:01:37] llamafactory.hparams.parser:384 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: None\n","[INFO|2025-04-11 10:01:37] llamafactory.hparams.parser:384 >> Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: None\n","[INFO|tokenization_utils_base.py:2313] 2025-04-11 10:01:37,711 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|2025-04-11 10:01:37] llamafactory.data.template:157 >> Add pad token: <|eot_id|>\n","[INFO|2025-04-11 10:01:37] llamafactory.data.template:157 >> Add <|eot_id|>,<|eom_id|> to stop words.\n","[INFO|2025-04-11 10:01:37] llamafactory.data.loader:157 >> Loading dataset output_cot_final.json...\n","Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n","Generating train split: 0 examples [00:00, ? examples/s][rank1]:[W411 10:01:38.862949103 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n","[rank2]:[W411 10:01:38.862963521 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n","Generating train split: 3307 examples [00:00, 7538.42 examples/s]\n","Converting format of dataset (num_proc=16): 100%|█| 3307/3307 [00:00<00:00, 1308\n","[rank0]:[W411 10:01:39.654175020 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n","Running tokenizer on dataset (num_proc=16): 100%|█| 3307/3307 [00:02<00:00, 1509\n","eval example:\n","input_ids:\n","[128000, 128006, 882, 128007, 271, 2127, 56956, 279, 2768, 27728, 1772, 323, 8417, 422, 433, 5727, 264, 4443, 2759, 315, 459, 31959, 13010, 311, 279, 559, 50703, 25474, 13, 2057, 10491, 11, 1833, 1521, 7504, 1473, 16, 13, 4343, 369, 1176, 25417, 3217, 57016, 320, 68, 1326, 2637, 1176, 29145, 19126, 60086, 1778, 439, 330, 40, 498, 330, 2465, 498, 477, 3230, 15407, 19392, 4443, 3217, 4390, 17, 13, 9372, 369, 459, 11720, 5224, 430, 279, 3927, 4036, 279, 559, 50703, 25474, 627, 18, 13, 65647, 904, 6420, 315, 3185, 6372, 477, 31959, 25481, 2768, 279, 47165, 627, 19, 13, 55215, 279, 2317, 25, 422, 279, 4221, 374, 55861, 477, 16632, 66836, 11, 16025, 7119, 264, 24790, 315, 364, 15, 30736, 6153, 13126, 1521, 3585, 11, 3493, 701, 4320, 439, 512, 12, 364, 16, 6, 422, 279, 1772, 9539, 5764, 264, 4443, 2759, 315, 31959, 25481, 627, 12, 364, 15, 6, 422, 433, 1587, 539, 627, 24901, 11, 2997, 264, 10015, 16540, 315, 701, 33811, 382, 7184, 11, 49229, 279, 2768, 1772, 28178, 627, 2059, 50703, 100058, 25, 27956, 11, 420, 1772, 5727, 459, 37036, 90687, 1295, 311, 1772, 8437, 710, 54950, 0, 358, 26968, 279, 559, 50703, 25474, 13, 578, 1176, 19660, 6612, 14470, 4725, 49043, 11, 7438, 358, 6612, 1234, 279, 9282, 369, 264, 1938, 719, 574, 16003, 13, 578, 2132, 832, 17551, 757, 704, 5128, 1695, 13, 358, 46498, 5115, 264, 2766, 279, 1828, 1938, 439, 422, 11039, 2555, 1022, 13, 358, 3077, 2533, 1766, 1198, 323, 12793, 482, 3293, 11470, 505, 279, 40409, 304, 279, 2326, 5605, 430, 4737, 19185, 97201, 31453, 11, 25689, 258, 477, 14221, 2963, 337, 1306, 4737, 264, 25474, 649, 48839, 14363, 6372, 2085, 48419, 4776, 279, 41265, 315, 279, 25474, 13, 358, 2744, 656, 433, 449, 279, 84117, 348, 710, 1457, 323, 358, 2846, 1695, 311, 733, 13, 128009, 128006, 78191, 128007, 271]\n","inputs:\n","<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n","\n","Analyze the following Reddit post and determine if it contains a personal account of an adverse reaction to the shingles vaccine. To decide, follow these steps:\n","\n","1. Check for first-hand experience cues (e.g., first-person pronouns such as \"I\", \"my\", or specific references indicating personal experience).\n","2. Look for an explicit statement that the individual received the shingles vaccine.\n","3. Identify any mention of side effects or adverse reactions following the vaccination.\n","4. Evaluate the context: if the language is ambiguous or merely speculative, lean towards a classification of '0'.\n","\n","After considering these points, provide your answer as:\n","- '1' if the post clearly includes a personal account of adverse reactions.\n","- '0' if it does not.\n","Finally, include a brief explanation of your reasoning.\n","\n","Now, classify the following post accordingly.\n","Shingles Vaccine: Warning, this post contains an optimistic antidote to post-vax sickness! I survived the shingles vaccine. The first dose felt fairly normal afterward, meaning I felt under the weather for a day but was functional. The second one laid me out pretty good. I slept quite a bit the next day as if fighting something off. I've since found -- and tested - recent instructions from the CDC in the US saying that taking ibuprofen, aspirin or Tylenol after taking a vaccine can relieve sick effects without hindering the efficacy of the vaccine. I always do it with the covid vax now and I'm good to go.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","\n","label_ids:\n","[16, 128009]\n","labels:\n","1<|eot_id|>\n","[INFO|configuration_utils.py:697] 2025-04-11 10:01:42,649 >> loading configuration file Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-04-11 10:01:42,650 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|2025-04-11 10:01:42] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n","[INFO|modeling_utils.py:3979] 2025-04-11 10:01:42,887 >> loading weights file Meta-Llama-3.1-8B-Instruct/model.safetensors.index.json\n","[INFO|modeling_utils.py:1633] 2025-04-11 10:01:42,888 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n","[INFO|configuration_utils.py:1140] 2025-04-11 10:01:42,890 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ]\n","}\n","\n","Loading checkpoint shards: 100%|██████████████████| 4/4 [02:46<00:00, 41.70s/it]\n","Loading checkpoint shards: 100%|██████████████████| 4/4 [02:46<00:00, 41.70s/it]\n","[INFO|modeling_utils.py:4970] 2025-04-11 10:04:30,278 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","[INFO|modeling_utils.py:4978] 2025-04-11 10:04:30,278 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at Meta-Llama-3.1-8B-Instruct.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","Loading checkpoint shards: 100%|██████████████████| 4/4 [02:46<00:00, 41.70s/it]\n","[INFO|configuration_utils.py:1093] 2025-04-11 10:04:30,482 >> loading configuration file Meta-Llama-3.1-8B-Instruct/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-04-11 10:04:30,482 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","[INFO|2025-04-11 10:04:30] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n","[INFO|2025-04-11 10:04:35] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n","[INFO|2025-04-11 10:04:35] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Llama-3.1-8B-Instruct/lora/train_cot_final_20250410\n","[INFO|2025-04-11 10:04:35] llamafactory.model.loader:157 >> all params: 8,030,261,248\n","Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n","[WARNING|2025-04-11 10:04:35] llamafactory.train.sft.workflow:168 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.\n","[INFO|trainer.py:4258] 2025-04-11 10:04:35,265 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4260] 2025-04-11 10:04:35,265 >>   Num examples = 3307\n","[INFO|trainer.py:4263] 2025-04-11 10:04:35,265 >>   Batch size = 2\n","100%|█████████████████████████████████████████| 552/552 [00:59<00:00,  6.26it/s]Building prefix dict from the default dictionary ...\n","Building prefix dict from the default dictionary ...\n","Building prefix dict from the default dictionary ...\n","Dumping model to file cache /tmp/jieba.cache\n","Dumping model to file cache /tmp/jieba.cache\n","Dumping model to file cache /tmp/jieba.cache\n","Loading model cost 0.508 seconds.\n","Prefix dict has been built successfully.\n","Loading model cost 0.505 seconds.\n","Prefix dict has been built successfully.\n","Loading model cost 0.510 seconds.\n","Prefix dict has been built successfully.\n","100%|█████████████████████████████████████████| 552/552 [01:00<00:00,  9.09it/s]\n","***** predict metrics *****\n","  predict_bleu-4                 =    35.3233\n","  predict_model_preparation_time =     0.0034\n","  predict_rouge-1                =    99.9094\n","  predict_rouge-2                =        0.0\n","  predict_rouge-l                =    99.9094\n","  predict_runtime                = 0:01:10.20\n","  predict_samples_per_second     =     47.102\n","  predict_steps_per_second       =      7.862\n","[INFO|2025-04-11 10:05:45] llamafactory.train.sft.trainer:157 >> Saving prediction results to saves/LLaMA3-8B/lora/predict_train_cot_final/generated_predictions.jsonl\n","[rank0]:[W411 10:05:50.741776888 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"]}]},{"cell_type":"code","source":["%cd /home1/09986/dchen1616/work/LLaMA-Factory\n","!CUDA_VISIBLE_DEVICES=0,1,2\n","!llamafactory-cli train \\\n","    --stage sft \\\n","    --do_predict \\\n","    --model_name_or_path \"Meta-Llama-3.1-8B-Instruct\" \\\n","    --adapter_name_or_path \"saves/Llama-3.1-8B-Instruct/lora/train_cot_final_20250410\"  \\\n","    --eval_dataset output_cot_test_final \\\n","    --dataset_dir ./data/New \\\n","    --template llama3 \\\n","    --finetuning_type lora \\\n","    --output_dir \"saves/LLaMA3-8B/lora/predict_cot_final\" \\\n","    --overwrite_cache \\\n","    --overwrite_output_dir \\\n","    --cutoff_len 2048 \\\n","    --preprocessing_num_workers 16 \\\n","    --per_device_eval_batch_size 2 \\\n","    --max_samples 100000 \\\n","    --predict_with_generate\n"],"metadata":{"id":"9uIaNllbclg3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744384410418,"user_tz":300,"elapsed":231111,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"}},"outputId":"3d24e648-63d1-4eff-88d0-4104c2d99f6b","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/work/09986/dchen1616/LLaMA-Factory\n","[INFO|2025-04-11 10:09:51] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:27779\n","[WARNING|2025-04-11 10:10:00] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n","[INFO|2025-04-11 10:10:00] llamafactory.hparams.parser:384 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: None\n","[INFO|configuration_utils.py:697] 2025-04-11 10:10:00,463 >> loading configuration file Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-04-11 10:10:00,464 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:10:00,466 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:10:00,466 >> loading file tokenizer.model\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:10:00,466 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:10:00,466 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:10:00,466 >> loading file tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:10:00,466 >> loading file chat_template.jinja\n","[INFO|tokenization_utils_base.py:2313] 2025-04-11 10:10:00,753 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|configuration_utils.py:697] 2025-04-11 10:10:00,755 >> loading configuration file Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-04-11 10:10:00,756 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:10:00,758 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:10:00,758 >> loading file tokenizer.model\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:10:00,758 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:10:00,758 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:10:00,758 >> loading file tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2048] 2025-04-11 10:10:00,758 >> loading file chat_template.jinja\n","[INFO|2025-04-11 10:10:00] llamafactory.hparams.parser:384 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: None\n","[INFO|tokenization_utils_base.py:2313] 2025-04-11 10:10:01,027 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|2025-04-11 10:10:01] llamafactory.data.template:157 >> Add pad token: <|eot_id|>\n","[INFO|2025-04-11 10:10:01] llamafactory.data.template:157 >> Add <|eot_id|>,<|eom_id|> to stop words.\n","[INFO|2025-04-11 10:10:01] llamafactory.data.loader:157 >> Loading dataset output_cot_test_final.json...\n","[INFO|2025-04-11 10:10:01] llamafactory.hparams.parser:384 >> Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: None\n","Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n","Generating train split: 0 examples [00:00, ? examples/s][rank1]:[W411 10:10:01.995391200 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n","Generating train split: 8113 examples [00:00, 46575.89 examples/s]\n","Converting format of dataset (num_proc=16):   0%| | 0/8113 [00:00<?, ? examples/[rank2]:[W411 10:10:01.207583879 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n","Converting format of dataset (num_proc=16): 100%|█| 8113/8113 [00:00<00:00, 2149\n","[rank0]:[W411 10:10:02.769478520 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n","Running tokenizer on dataset (num_proc=16):   6%| | 508/8113 [00:00<00:12, 610.3[WARNING|tokenization_utils_base.py:3945] 2025-04-11 10:10:04,803 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2993 > 2048). Running this sequence through the model will result in indexing errors\n","Running tokenizer on dataset (num_proc=16):  38%|▍| 3043/8113 [00:01<00:01, 3063[WARNING|tokenization_utils_base.py:3945] 2025-04-11 10:10:05,300 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2080 > 2048). Running this sequence through the model will result in indexing errors\n","Running tokenizer on dataset (num_proc=16):  50%|▌| 4057/8113 [00:01<00:00, 4134[WARNING|tokenization_utils_base.py:3945] 2025-04-11 10:10:05,407 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2105 > 2048). Running this sequence through the model will result in indexing errors\n","Running tokenizer on dataset (num_proc=16):  63%|▋| 5071/8113 [00:01<00:00, 4488[WARNING|tokenization_utils_base.py:3945] 2025-04-11 10:10:05,682 >> Token indices sequence length is longer than the specified maximum sequence length for this model (3997 > 2048). Running this sequence through the model will result in indexing errors\n","Running tokenizer on dataset (num_proc=16):  88%|▉| 7099/8113 [00:02<00:00, 4946[WARNING|tokenization_utils_base.py:3945] 2025-04-11 10:10:05,981 >> Token indices sequence length is longer than the specified maximum sequence length for this model (6229 > 2048). Running this sequence through the model will result in indexing errors\n","Running tokenizer on dataset (num_proc=16): 100%|█| 8113/8113 [00:02<00:00, 3377\n","eval example:\n","input_ids:\n","[128000, 128006, 882, 128007, 271, 2127, 56956, 279, 2768, 27728, 1772, 323, 8417, 422, 433, 5727, 264, 4443, 2759, 315, 459, 31959, 13010, 311, 279, 559, 50703, 25474, 13, 2057, 10491, 11, 1833, 1521, 7504, 1473, 16, 13, 4343, 369, 1176, 25417, 3217, 57016, 320, 68, 1326, 2637, 1176, 29145, 19126, 60086, 1778, 439, 330, 40, 498, 330, 2465, 498, 477, 3230, 15407, 19392, 4443, 3217, 4390, 17, 13, 9372, 369, 459, 11720, 5224, 430, 279, 3927, 4036, 279, 559, 50703, 25474, 627, 18, 13, 65647, 904, 6420, 315, 3185, 6372, 477, 31959, 25481, 2768, 279, 47165, 627, 19, 13, 55215, 279, 2317, 25, 422, 279, 4221, 374, 55861, 477, 16632, 66836, 11, 16025, 7119, 264, 24790, 315, 364, 15, 30736, 6153, 13126, 1521, 3585, 11, 3493, 701, 4320, 439, 512, 12, 364, 16, 6, 422, 279, 1772, 9539, 5764, 264, 4443, 2759, 315, 31959, 25481, 627, 12, 364, 15, 6, 422, 433, 1587, 539, 627, 24901, 11, 2997, 264, 10015, 16540, 315, 701, 33811, 382, 7184, 11, 49229, 279, 2768, 1772, 28178, 627, 10156, 2751, 856, 20236, 6689, 1566, 2305, 323, 1457, 358, 2846, 25051, 1063, 15234, 13803, 13, 358, 3077, 1027, 8430, 2216, 8834, 343, 3340, 323, 34361, 88, 11, 4661, 1093, 358, 617, 279, 20236, 1578, 13, 11697, 5606, 775, 10534, 420, 1306, 3794, 279, 20236, 25474, 30, 128009, 128006, 78191, 128007, 271]\n","inputs:\n","<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n","\n","Analyze the following Reddit post and determine if it contains a personal account of an adverse reaction to the shingles vaccine. To decide, follow these steps:\n","\n","1. Check for first-hand experience cues (e.g., first-person pronouns such as \"I\", \"my\", or specific references indicating personal experience).\n","2. Look for an explicit statement that the individual received the shingles vaccine.\n","3. Identify any mention of side effects or adverse reactions following the vaccination.\n","4. Evaluate the context: if the language is ambiguous or merely speculative, lean towards a classification of '0'.\n","\n","After considering these points, provide your answer as:\n","- '1' if the post clearly includes a personal account of adverse reactions.\n","- '0' if it does not.\n","Finally, include a brief explanation of your reasoning.\n","\n","Now, classify the following post accordingly.\n","Just got my flu shot last month and now I'm experiencing some strange symptoms. I've been feeling really fatigued and achy, almost like I have the flu again. Has anyone else experienced this after getting the flu vaccine?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","\n","label_ids:\n","[128009]\n","labels:\n","<|eot_id|>\n","[INFO|configuration_utils.py:697] 2025-04-11 10:10:06,389 >> loading configuration file Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-04-11 10:10:06,390 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|2025-04-11 10:10:06] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n","[INFO|modeling_utils.py:3979] 2025-04-11 10:10:06,512 >> loading weights file Meta-Llama-3.1-8B-Instruct/model.safetensors.index.json\n","[INFO|modeling_utils.py:1633] 2025-04-11 10:10:06,513 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n","[INFO|configuration_utils.py:1140] 2025-04-11 10:10:06,515 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ]\n","}\n","\n","Loading checkpoint shards: 100%|██████████████████| 4/4 [00:10<00:00,  2.55s/it]\n","[INFO|modeling_utils.py:4970] 2025-04-11 10:10:16,770 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","[INFO|modeling_utils.py:4978] 2025-04-11 10:10:16,770 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at Meta-Llama-3.1-8B-Instruct.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","Loading checkpoint shards: 100%|██████████████████| 4/4 [00:10<00:00,  2.52s/it]\n","[INFO|configuration_utils.py:1093] 2025-04-11 10:10:16,781 >> loading configuration file Meta-Llama-3.1-8B-Instruct/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-04-11 10:10:16,781 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","[INFO|2025-04-11 10:10:16] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n","Loading checkpoint shards: 100%|██████████████████| 4/4 [00:10<00:00,  2.55s/it]\n","[INFO|2025-04-11 10:10:17] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n","[INFO|2025-04-11 10:10:17] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Llama-3.1-8B-Instruct/lora/train_cot_final_20250410\n","[INFO|2025-04-11 10:10:17] llamafactory.model.loader:157 >> all params: 8,030,261,248\n","Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n","[WARNING|2025-04-11 10:10:17] llamafactory.train.sft.workflow:168 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.\n","[INFO|trainer.py:4258] 2025-04-11 10:10:17,597 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4260] 2025-04-11 10:10:17,597 >>   Num examples = 8113\n","[INFO|trainer.py:4263] 2025-04-11 10:10:17,597 >>   Batch size = 2\n","100%|███████████████████████████████████████| 1353/1353 [02:55<00:00,  5.80it/s]Building prefix dict from the default dictionary ...\n","Loading model from cache /tmp/jieba.cache\n","Building prefix dict from the default dictionary ...\n","Loading model from cache /tmp/jieba.cache\n","Building prefix dict from the default dictionary ...\n","Loading model from cache /tmp/jieba.cache\n","Loading model cost 0.474 seconds.\n","Prefix dict has been built successfully.\n","Loading model cost 0.475 seconds.\n","Prefix dict has been built successfully.\n","Loading model cost 0.481 seconds.\n","Prefix dict has been built successfully.\n","100%|███████████████████████████████████████| 1353/1353 [02:57<00:00,  7.62it/s]\n","***** predict metrics *****\n","  predict_bleu-4                 =        0.0\n","  predict_model_preparation_time =      0.003\n","  predict_rouge-1                =        0.0\n","  predict_rouge-2                =        0.0\n","  predict_rouge-l                =        0.0\n","  predict_runtime                = 0:02:59.07\n","  predict_samples_per_second     =     45.304\n","  predict_steps_per_second       =      7.555\n","[INFO|2025-04-11 10:13:16] llamafactory.train.sft.trainer:157 >> Saving prediction results to saves/LLaMA3-8B/lora/predict_cot_final/generated_predictions.jsonl\n","[rank0]:[W411 10:13:28.599839580 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"]}]},{"cell_type":"markdown","source":["# Prediction of training and validation dataset W/O sft"],"metadata":{"id":"iygXSCVm8yEi"}},{"cell_type":"code","source":["%cd /home1/09986/dchen1616/work/LLaMA-Factory\n","!CUDA_VISIBLE_DEVICES=0,1,2\n","!llamafactory-cli train \\\n","    --stage sft \\\n","    --do_predict \\\n","    --model_name_or_path \"Meta-Llama-3.1-8B-Instruct\" \\\n","    --eval_dataset output \\\n","    --dataset_dir ./data \\\n","    --template llama3 \\\n","    --finetuning_type lora \\\n","    --output_dir \"saves/LLaMA3-8B/lora/predict_train_wo_sft\" \\\n","    --overwrite_cache \\\n","    --overwrite_output_dir \\\n","    --cutoff_len 2048 \\\n","    --preprocessing_num_workers 16 \\\n","    --per_device_eval_batch_size 2 \\\n","    --max_samples 100000 \\\n","    --predict_with_generate"],"metadata":{"id":"TbBPAiC9c09Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741119889931,"user_tz":360,"elapsed":759797,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"}},"outputId":"d14c520a-9c69-49eb-f5d3-efd2ef75bf4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/work/09986/dchen1616/LLaMA-Factory\n","[INFO|2025-03-04 14:12:20] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:21162\n","[WARNING|2025-03-04 14:12:31] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n","[INFO|2025-03-04 14:12:31] llamafactory.hparams.parser:384 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: None\n","[INFO|configuration_utils.py:697] 2025-03-04 14:12:31,033 >> loading configuration file Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-03-04 14:12:31,033 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:12:31,035 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:12:31,035 >> loading file tokenizer.model\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:12:31,035 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:12:31,035 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:12:31,035 >> loading file tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:12:31,035 >> loading file chat_template.jinja\n","[INFO|tokenization_utils_base.py:2313] 2025-03-04 14:12:31,297 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|configuration_utils.py:697] 2025-03-04 14:12:31,300 >> loading configuration file Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-03-04 14:12:31,301 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:12:31,302 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:12:31,302 >> loading file tokenizer.model\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:12:31,302 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:12:31,302 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:12:31,302 >> loading file tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:12:31,302 >> loading file chat_template.jinja\n","[INFO|2025-03-04 14:12:31] llamafactory.hparams.parser:384 >> Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: None\n","[INFO|2025-03-04 14:12:31] llamafactory.hparams.parser:384 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: None\n","[INFO|tokenization_utils_base.py:2313] 2025-03-04 14:12:31,563 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|2025-03-04 14:12:31] llamafactory.data.template:157 >> Add pad token: <|eot_id|>\n","[INFO|2025-03-04 14:12:31] llamafactory.data.template:157 >> Add <|eot_id|>,<|eom_id|> to stop words.\n","[INFO|2025-03-04 14:12:31] llamafactory.data.loader:157 >> Loading dataset output.json...\n","Converting format of dataset (num_proc=16):   0%| | 0/2521 [00:00<?, ? examples/[rank2]:[W304 14:12:31.036850630 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n","[rank1]:[W304 14:12:31.049713128 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n","Converting format of dataset (num_proc=16): 100%|█| 2521/2521 [00:00<00:00, 1102\n","[rank0]:[W304 14:12:32.472101835 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n","Running tokenizer on dataset (num_proc=16): 100%|█| 2521/2521 [00:02<00:00, 1100\n","eval example:\n","input_ids:\n","[128000, 128006, 882, 128007, 271, 5005, 579, 422, 264, 27728, 1772, 374, 264, 4443, 2759, 315, 31959, 25474, 25481, 320, 52340, 750, 369, 279, 559, 50703, 25474, 4390, 32, 4443, 31959, 13010, 6420, 3445, 4423, 374, 23524, 872, 1866, 477, 2500, 3927, 596, 3185, 6372, 1306, 279, 6689, 627, 2746, 779, 11, 4320, 364, 16, 7112, 422, 539, 11, 4320, 364, 15, 3961, 41481, 512, 16, 13, 3962, 25, 364, 10156, 2751, 856, 2132, 559, 50703, 25474, 323, 36765, 11, 1148, 264, 12141, 0, 578, 1176, 832, 574, 264, 46385, 11, 719, 420, 892, 358, 1047, 682, 279, 61794, 323, 421, 67043, 315, 3185, 6372, 13, 435, 14437, 28129, 430, 433, 596, 682, 5922, 433, 304, 279, 842, 32483, 11651, 220, 16, 198, 17, 13, 3962, 25, 364, 3923, 264, 27328, 66, 7232, 0, 23804, 287, 279, 1443, 50703, 54082, 1457, 0, 1443, 50703, 832, 315, 279, 3967, 3185, 6372, 315, 279, 20562, 54082, 3238, 11651, 220, 15, 271, 7184, 49229, 279, 2768, 1772, 28178, 512, 2059, 50703, 100058, 25, 27956, 11, 420, 1772, 5727, 459, 37036, 90687, 1295, 311, 1772, 8437, 710, 54950, 0, 358, 26968, 279, 559, 50703, 25474, 13, 578, 1176, 19660, 6612, 14470, 4725, 49043, 11, 7438, 358, 6612, 1234, 279, 9282, 369, 264, 1938, 719, 574, 16003, 13, 578, 2132, 832, 17551, 757, 704, 5128, 1695, 13, 358, 46498, 5115, 264, 2766, 279, 1828, 1938, 439, 422, 11039, 2555, 1022, 13, 358, 3077, 2533, 1766, 1198, 323, 12793, 482, 3293, 11470, 505, 279, 40409, 304, 279, 2326, 5605, 430, 4737, 19185, 97201, 31453, 11, 25689, 258, 477, 14221, 2963, 337, 1306, 4737, 264, 25474, 649, 48839, 14363, 6372, 2085, 48419, 4776, 279, 41265, 315, 279, 25474, 13, 358, 2744, 656, 433, 449, 279, 84117, 348, 710, 1457, 323, 358, 2846, 1695, 311, 733, 13, 128009, 128006, 78191, 128007, 271]\n","inputs:\n","<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n","\n","Decide if a Reddit post is a personal account of adverse vaccine reactions (specifically for the shingles vaccine).\n","A personal adverse reaction mention means someone is describing their own or another individual's side effects after the shot.\n","If so, answer '1'; if not, answer '0'\n","\n","Examples:\n","1. Post: 'Just got my second shingles vaccine and wow, what a ride! The first one was a breeze, but this time I had all the bells and whistles of side effects. Fingers crossed that it's all worth it in the end!' → 1\n","2. Post: 'What a Coincidence! Pushing the Shingles Jab now! Shingles one of the known side effects of the COVID Jab.' → 0\n","\n","Now classify the following post accordingly:\n","Shingles Vaccine: Warning, this post contains an optimistic antidote to post-vax sickness! I survived the shingles vaccine. The first dose felt fairly normal afterward, meaning I felt under the weather for a day but was functional. The second one laid me out pretty good. I slept quite a bit the next day as if fighting something off. I've since found -- and tested - recent instructions from the CDC in the US saying that taking ibuprofen, aspirin or Tylenol after taking a vaccine can relieve sick effects without hindering the efficacy of the vaccine. I always do it with the covid vax now and I'm good to go.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","\n","label_ids:\n","[16, 128009]\n","labels:\n","1<|eot_id|>\n","[INFO|configuration_utils.py:697] 2025-03-04 14:12:35,542 >> loading configuration file Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-03-04 14:12:35,543 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|2025-03-04 14:12:35] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n","[INFO|modeling_utils.py:3979] 2025-03-04 14:12:35,673 >> loading weights file Meta-Llama-3.1-8B-Instruct/model.safetensors.index.json\n","[INFO|modeling_utils.py:1633] 2025-03-04 14:12:35,674 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n","[INFO|configuration_utils.py:1140] 2025-03-04 14:12:35,676 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ]\n","}\n","\n","Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.35s/it]\n","[INFO|modeling_utils.py:4970] 2025-03-04 14:12:41,112 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","[INFO|modeling_utils.py:4978] 2025-03-04 14:12:41,112 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at Meta-Llama-3.1-8B-Instruct.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","[INFO|configuration_utils.py:1093] 2025-03-04 14:12:41,125 >> loading configuration file Meta-Llama-3.1-8B-Instruct/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-04 14:12:41,125 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","[INFO|2025-03-04 14:12:41] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n","[INFO|2025-03-04 14:12:41] llamafactory.model.loader:157 >> all params: 8,030,261,248\n","Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n","[WARNING|2025-03-04 14:12:41] llamafactory.train.sft.workflow:168 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.\n","[INFO|trainer.py:4258] 2025-03-04 14:12:41,150 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4260] 2025-03-04 14:12:41,150 >>   Num examples = 2521\n","[INFO|trainer.py:4263] 2025-03-04 14:12:41,150 >>   Batch size = 2\n","Loading checkpoint shards: 100%|██████████████████| 4/4 [00:06<00:00,  1.55s/it]\n","Loading checkpoint shards: 100%|██████████████████| 4/4 [00:07<00:00,  1.80s/it]\n","100%|█████████████████████████████████████████| 421/421 [11:56<00:00,  2.04s/it]Building prefix dict from the default dictionary ...\n","Loading model from cache /tmp/jieba.cache\n","Building prefix dict from the default dictionary ...\n","Loading model from cache /tmp/jieba.cache\n","Building prefix dict from the default dictionary ...\n","Loading model from cache /tmp/jieba.cache\n","Loading model cost 0.482 seconds.\n","Prefix dict has been built successfully.\n","Loading model cost 0.481 seconds.\n","Prefix dict has been built successfully.\n","Loading model cost 0.486 seconds.\n","Prefix dict has been built successfully.\n","100%|█████████████████████████████████████████| 421/421 [11:58<00:00,  1.71s/it]\n","***** predict metrics *****\n","  predict_bleu-4                 =     3.6238\n","  predict_model_preparation_time =     0.0032\n","  predict_rouge-1                =    13.8243\n","  predict_rouge-2                =        0.0\n","  predict_rouge-l                =    13.3382\n","  predict_runtime                = 0:12:02.83\n","  predict_samples_per_second     =      3.488\n","  predict_steps_per_second       =      0.582\n","[INFO|2025-03-04 14:24:43] llamafactory.train.sft.trainer:157 >> Saving prediction results to saves/LLaMA3-8B/lora/predict_train_wo_sft/generated_predictions.jsonl\n","[rank0]:[W304 14:24:47.655372404 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"]}]},{"cell_type":"code","source":["%cd /home1/09986/dchen1616/work/LLaMA-Factory\n","!CUDA_VISIBLE_DEVICES=0,1,2\n","!llamafactory-cli train \\\n","    --stage sft \\\n","    --do_predict \\\n","    --model_name_or_path \"Meta-Llama-3.1-8B-Instruct\" \\\n","    --eval_dataset output_validate \\\n","    --dataset_dir ./data \\\n","    --template llama3 \\\n","    --finetuning_type lora \\\n","    --output_dir \"saves/LLaMA3-8B/lora/predict_wo_sft\" \\\n","    --overwrite_cache \\\n","    --overwrite_output_dir \\\n","    --cutoff_len 2048 \\\n","    --preprocessing_num_workers 16 \\\n","    --per_device_eval_batch_size 2 \\\n","    --max_samples 100000 \\\n","    --predict_with_generate"],"metadata":{"id":"_OX_9WW5c8UP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741120142148,"user_tz":360,"elapsed":250758,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"}},"outputId":"e753ba25-fdc8-4eb4-ec0e-aa05a3ba5c8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/work/09986/dchen1616/LLaMA-Factory\n","[INFO|2025-03-04 14:25:01] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:21601\n","[WARNING|2025-03-04 14:25:10] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n","[INFO|2025-03-04 14:25:10] llamafactory.hparams.parser:384 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: None\n","[INFO|configuration_utils.py:697] 2025-03-04 14:25:10,902 >> loading configuration file Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-03-04 14:25:10,903 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:25:10,904 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:25:10,904 >> loading file tokenizer.model\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:25:10,905 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:25:10,905 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:25:10,905 >> loading file tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:25:10,905 >> loading file chat_template.jinja\n","[INFO|tokenization_utils_base.py:2313] 2025-03-04 14:25:11,167 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|configuration_utils.py:697] 2025-03-04 14:25:11,169 >> loading configuration file Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-03-04 14:25:11,170 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:25:11,171 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:25:11,171 >> loading file tokenizer.model\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:25:11,171 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:25:11,171 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:25:11,171 >> loading file tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2048] 2025-03-04 14:25:11,171 >> loading file chat_template.jinja\n","[INFO|2025-03-04 14:25:11] llamafactory.hparams.parser:384 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: None\n","[INFO|2025-03-04 14:25:11] llamafactory.hparams.parser:384 >> Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: None\n","[INFO|tokenization_utils_base.py:2313] 2025-03-04 14:25:11,434 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|2025-03-04 14:25:11] llamafactory.data.template:157 >> Add pad token: <|eot_id|>\n","[INFO|2025-03-04 14:25:11] llamafactory.data.template:157 >> Add <|eot_id|>,<|eom_id|> to stop words.\n","[INFO|2025-03-04 14:25:11] llamafactory.data.loader:157 >> Loading dataset output_validate.json...\n","[rank1]:[W304 14:25:11.823461552 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n","Converting format of dataset (num_proc=16):   0%| | 0/786 [00:00<?, ? examples/s[rank2]:[W304 14:25:11.896559166 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n","Converting format of dataset (num_proc=16): 100%|█| 786/786 [00:00<00:00, 4405.7\n","[rank0]:[W304 14:25:12.299912663 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n","Running tokenizer on dataset (num_proc=16): 100%|█| 786/786 [00:02<00:00, 373.14\n","eval example:\n","input_ids:\n","[128000, 128006, 882, 128007, 271, 5005, 579, 422, 264, 27728, 1772, 374, 264, 4443, 2759, 315, 31959, 25474, 25481, 320, 52340, 750, 369, 279, 559, 50703, 25474, 4390, 32, 4443, 31959, 13010, 6420, 3445, 4423, 374, 23524, 872, 1866, 477, 2500, 3927, 596, 3185, 6372, 1306, 279, 6689, 627, 2746, 779, 11, 4320, 364, 16, 7112, 422, 539, 11, 4320, 364, 15, 3961, 41481, 512, 16, 13, 3962, 25, 364, 10156, 2751, 856, 2132, 559, 50703, 25474, 323, 36765, 11, 1148, 264, 12141, 0, 578, 1176, 832, 574, 264, 46385, 11, 719, 420, 892, 358, 1047, 682, 279, 61794, 323, 421, 67043, 315, 3185, 6372, 13, 435, 14437, 28129, 430, 433, 596, 682, 5922, 433, 304, 279, 842, 32483, 11651, 220, 16, 198, 17, 13, 3962, 25, 364, 3923, 264, 27328, 66, 7232, 0, 23804, 287, 279, 1443, 50703, 54082, 1457, 0, 1443, 50703, 832, 315, 279, 3967, 3185, 6372, 315, 279, 20562, 54082, 3238, 11651, 220, 15, 271, 7184, 49229, 279, 2768, 1772, 28178, 512, 16, 267, 559, 50703, 6689, 927, 220, 5332, 4207, 4227, 323, 2103, 8834, 343, 3340, 25, 10657, 6689, 40199, 2288, 1975, 128009, 128006, 78191, 128007, 271]\n","inputs:\n","<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n","\n","Decide if a Reddit post is a personal account of adverse vaccine reactions (specifically for the shingles vaccine).\n","A personal adverse reaction mention means someone is describing their own or another individual's side effects after the shot.\n","If so, answer '1'; if not, answer '0'\n","\n","Examples:\n","1. Post: 'Just got my second shingles vaccine and wow, what a ride! The first one was a breeze, but this time I had all the bells and whistles of side effects. Fingers crossed that it's all worth it in the end!' → 1\n","2. Post: 'What a Coincidence! Pushing the Shingles Jab now! Shingles one of the known side effects of the COVID Jab.' → 0\n","\n","Now classify the following post accordingly:\n","1st shingles shot over 72 hours ago and still fatigued: Second shot sucks too....<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","\n","label_ids:\n","[16, 128009]\n","labels:\n","1<|eot_id|>\n","[INFO|configuration_utils.py:697] 2025-03-04 14:25:15,197 >> loading configuration file Meta-Llama-3.1-8B-Instruct/config.json\n","[INFO|configuration_utils.py:771] 2025-03-04 14:25:15,198 >> Model config LlamaConfig {\n","  \"_name_or_path\": \"Meta-Llama-3.1-8B-Instruct\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|2025-03-04 14:25:15] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n","[INFO|modeling_utils.py:3979] 2025-03-04 14:25:15,338 >> loading weights file Meta-Llama-3.1-8B-Instruct/model.safetensors.index.json\n","[INFO|modeling_utils.py:1633] 2025-03-04 14:25:15,338 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n","[INFO|configuration_utils.py:1140] 2025-03-04 14:25:15,341 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ]\n","}\n","\n","Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.40s/it]\n","[INFO|modeling_utils.py:4970] 2025-03-04 14:25:20,980 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","[INFO|modeling_utils.py:4978] 2025-03-04 14:25:20,980 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at Meta-Llama-3.1-8B-Instruct.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","[INFO|configuration_utils.py:1093] 2025-03-04 14:25:20,990 >> loading configuration file Meta-Llama-3.1-8B-Instruct/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-03-04 14:25:20,990 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","[INFO|2025-03-04 14:25:20] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n","[INFO|2025-03-04 14:25:20] llamafactory.model.loader:157 >> all params: 8,030,261,248\n","Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n","[WARNING|2025-03-04 14:25:21] llamafactory.train.sft.workflow:168 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.\n","[INFO|trainer.py:4258] 2025-03-04 14:25:21,016 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4260] 2025-03-04 14:25:21,016 >>   Num examples = 786\n","[INFO|trainer.py:4263] 2025-03-04 14:25:21,016 >>   Batch size = 2\n","Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.38s/it]\n","Loading checkpoint shards: 100%|██████████████████| 4/4 [00:06<00:00,  1.72s/it]\n","100%|█████████████████████████████████████████| 131/131 [03:32<00:00,  2.01s/it]Building prefix dict from the default dictionary ...\n","Loading model from cache /tmp/jieba.cache\n","Building prefix dict from the default dictionary ...\n","Loading model from cache /tmp/jieba.cache\n","Building prefix dict from the default dictionary ...\n","Loading model from cache /tmp/jieba.cache\n","Loading model cost 0.475 seconds.\n","Prefix dict has been built successfully.\n","Loading model cost 0.479 seconds.\n","Prefix dict has been built successfully.\n","Loading model cost 0.482 seconds.\n","Prefix dict has been built successfully.\n","100%|█████████████████████████████████████████| 131/131 [03:34<00:00,  1.63s/it]\n","***** predict metrics *****\n","  predict_bleu-4                 =     3.8116\n","  predict_model_preparation_time =     0.0032\n","  predict_rouge-1                =    14.3298\n","  predict_rouge-2                =        0.0\n","  predict_rouge-l                =    13.8454\n","  predict_runtime                = 0:03:37.36\n","  predict_samples_per_second     =      3.616\n","  predict_steps_per_second       =      0.603\n","[INFO|2025-03-04 14:28:58] llamafactory.train.sft.trainer:157 >> Saving prediction results to saves/LLaMA3-8B/lora/predict_wo_sft/generated_predictions.jsonl\n","[rank0]:[W304 14:28:59.835522080 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"]}]},{"cell_type":"markdown","source":["# Confusion matrix evaluation for classification"],"metadata":{"id":"J21k3U065A2s"}},{"cell_type":"code","source":["import json\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n","#Performance with sft\n","#Training dataset performance\n","file_path = \"/home1/09986/dchen1616/work/LLaMA-Factory/saves/LLaMA3-8B/lora/predict_train_cot_final/generated_predictions.jsonl\"\n","y_true=[]\n","y_pred=[]\n","with open(file_path, 'r', encoding='utf-8') as f:\n","    for line in f:\n","        line = line.strip()\n","        if not line:\n","            continue\n","        data = json.loads(line)\n","        y_true.append(data['label'])\n","        y_pred.append(data['predict'])\n","cm = confusion_matrix(y_true, y_pred)\n","print(\"Performance with sft\")\n","print(\"Confusion Matrix of training dataset_cot:\")\n","print(cm)\n","acc = accuracy_score(y_true, y_pred)\n","print(\"Training Accuracy:\", acc)\n","prec = precision_score(y_true, y_pred, pos_label='1',average='binary')\n","print(\"Training Precision:\", prec)\n","rec = recall_score(y_true, y_pred, pos_label='1',average='binary')\n","print(\"Training Recall:\", rec)\n","f1 = f1_score(y_true, y_pred, pos_label='1',average='binary')\n","print(\"F1 Score (macro):\", f1)\n","print(\"Classification Report:\")\n","print(classification_report(y_true, y_pred))\n"],"metadata":{"id":"nM-zHlQHdEPN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744384490743,"user_tz":300,"elapsed":73,"user":{"displayName":"Celine Chen","userId":"00134181201670929484"}},"outputId":"62c59fd4-218d-4909-b009-006dd824c1c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Performance with sft\n","Confusion Matrix of training dataset_cot:\n","[[1792    0]\n"," [   3 1512]]\n","Training Accuracy: 0.9990928333837314\n","Training Precision: 1.0\n","Training Recall: 0.998019801980198\n","F1 Score (macro): 0.9990089197224975\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      1792\n","           1       1.00      1.00      1.00      1515\n","\n","    accuracy                           1.00      3307\n","   macro avg       1.00      1.00      1.00      3307\n","weighted avg       1.00      1.00      1.00      3307\n","\n"]}]},{"cell_type":"code","source":["#Validation dataset performance\n","file_path = \"/home1/09986/dchen1616/work/LLaMA-Factory/saves/LLaMA3-8B/lora/predict_cot_final/generated_predictions.jsonl\"\n","y_true=[]\n","y_pred=[]\n","with open(file_path, 'r', encoding='utf-8') as f:\n","    for line in f:\n","        line = line.strip()\n","        if not line:\n","            continue\n","        data = json.loads(line)\n","        y_true.append(data['label'])\n","        y_pred.append(data['predict'])\n","cm = confusion_matrix(y_true, y_pred)\n","print(\"\\nConfusion Matrix of validation dataset_cot:\")\n","print(cm)\n","acc = accuracy_score(y_true, y_pred)\n","print(\"Validation Accuracy:\", acc)\n","prec = precision_score(y_true, y_pred, pos_label='1',average='binary')\n","print(\"Validation Precision:\", prec)\n","rec = recall_score(y_true, y_pred, pos_label='1', average='binary')\n","print(\"Validation Recall:\", rec)\n","f1 = f1_score(y_true, y_pred, pos_label='1', average='binary')\n","print(\"F1 Score (macro):\", f1)\n","print(\"Classification Report:\")\n","print(classification_report(y_true, y_pred))"],"metadata":{"id":"fLxSj4GBawrE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Output Predicted labels and Mis-predicted samples"],"metadata":{"id":"s4PJ6-w7oiB6"}},{"cell_type":"code","source":["import json\n","import pandas as pd\n","file_path = \"/home1/09986/dchen1616/work/LLaMA-Factory/saves/LLaMA3-8B/lora/predict_cot_final/generated_predictions.jsonl\"\n","original_val_dataset=pd.read_csv('/home1/09986/dchen1616/work/LLaMA-Factory/data/New/test_task6.csv',index_col=False)\n","y_pred=[]\n","with open(file_path, 'r', encoding='utf-8') as f:\n","    for line in f:\n","        line = line.strip()\n","        if not line:\n","            continue\n","        data = json.loads(line)\n","        y_pred.append(data['predict'])\n","original_val_dataset.insert(loc=2,column='Prediction',value=y_pred)\n","original_val_dataset.to_csv('/home1/09986/dchen1616/work/LLaMA-Factory/data/New/Test_Result_Task6.csv',index=False)\n","\n","# rm_list=[]\n","# original_val_dataset\n","# for line_id,line in enumerate(original_val_dataset['Prediction']):\n","#     if str(original_val_dataset['labels'][line_id])==str(line):\n","#       rm_list.append(line_id)\n","# original_val_dataset=original_val_dataset.drop(rm_list).reset_index(drop=True)\n","# original_val_dataset.to_csv('/home1/09986/dchen1616/work/LLaMA-Factory/data/New/cot_valid_Miss_Prediction_Task6.csv',index=False)"],"metadata":{"id":"k_vevuTnoeFB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import pandas as pd\n","file_path = \"/home1/09986/dchen1616/work/LLaMA-Factory/saves/LLaMA3-8B/lora/predict_train_cot_update/generated_predictions.jsonl\"\n","original_val_dataset=pd.read_csv('/home1/09986/dchen1616/work/LLaMA-Factory/data/train 1_Task6.csv',index_col=False)\n","y_pred=[]\n","with open(file_path, 'r', encoding='utf-8') as f:\n","    for line in f:\n","        line = line.strip()\n","        if not line:\n","            continue\n","        data = json.loads(line)\n","        y_pred.append(data['predict'])\n","original_val_dataset.insert(loc=3,column='Prediction',value=y_pred)\n","original_val_dataset.to_csv('/home1/09986/dchen1616/work/LLaMA-Factory/data/Train_Miss_Prediction_cot_update.csv',index=False)\n","rm_list=[]\n","original_val_dataset\n","for line_id,line in enumerate(original_val_dataset['Prediction']):\n","    if str(original_val_dataset['labels'][line_id])==str(line):\n","      rm_list.append(line_id)\n","original_val_dataset=original_val_dataset.drop(rm_list).reset_index(drop=True)\n","original_val_dataset.to_csv('/home1/09986/dchen1616/work/LLaMA-Factory/data/Train_Miss_Prediction_cot_update.csv',index=False)"],"metadata":{"id":"E1B1v8J-mbsd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prediction for the test set\n"],"metadata":{"id":"s-aB2P9FpfkA"}},{"cell_type":"code","source":["import json\n","import pandas as pd\n","file_path = \"/home1/09986/dchen1616/work/LLaMA-Factory/saves/LLaMA3-8B/lora/predict_cot_new/generated_predictions.jsonl\"\n","original_val_dataset=pd.read_csv('/home1/09986/dchen1616/work/LLaMA-Factory/data/New/valid 1_Task6.csv',index_col=False)\n","y_pred=[]\n","with open(file_path, 'r', encoding='utf-8') as f:\n","    for line in f:\n","        line = line.strip()\n","        if not line:\n","            continue\n","        data = json.loads(line)\n","        y_pred.append(data['predict'])\n","original_val_dataset.insert(loc=3,column='Prediction',value=y_pred)\n","original_val_dataset.to_csv('/home1/09986/dchen1616/work/LLaMA-Factory/data/New/cot_valid_Prediction_Task6.csv',index=False)\n","rm_list=[]\n","original_val_dataset\n","for line_id,line in enumerate(original_val_dataset['Prediction']):\n","    if str(original_val_dataset['labels'][line_id])==str(line):\n","      rm_list.append(line_id)\n","original_val_dataset=original_val_dataset.drop(rm_list).reset_index(drop=True)\n","original_val_dataset.to_csv('/home1/09986/dchen1616/work/LLaMA-Factory/data/New/cot_valid_Miss_Prediction_Task6.csv',index=False)"],"metadata":{"id":"fBk9tm1BpesI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---------------------------end line-------------------------------------------"],"metadata":{"id":"MhpjAM_YdTWz"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"16RCgjNVwSrEBwbhTHXGCKhvyNn9R3iX0","timestamp":1721918612885},{"file_id":"1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9","timestamp":1721491368507}],"collapsed_sections":["lr7rB3szzhtx","jtbGS0mL4YoT","iygXSCVm8yEi"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}